import os
import requests
from datetime import datetime
import time
import re
from urllib.parse import quote

def fetch_reddit_pain_points(subreddit):
    """
    æŠ“å–æŒ‡å®š Subreddit çš„æ½œåœ¨ç—›ç‚¹è´´
    """
    # å…³é”®è¯ç»„åˆï¼šå¯»æ‰¾â€œå›°éš¾â€ã€â€œæ‰‹åŠ¨â€ã€â€œå¯»æ‰¾Appâ€ã€â€œç°æœ‰å·¥å…·ç¼ºé¡¹â€
    keywords = "(manual OR 'hard to' OR 'is there an app' OR 'alternative to' OR 'problem' OR 'frusting')"
    url = f"https://www.reddit.com/r/{subreddit}/search.json?q={keywords}&restrict_sr=1&sort=new&t=week"

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    try:
        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            return f"### [!] æ— æ³•è®¿é—® r/{subreddit} (HTTP {response.status_code})\n\n"

        data = response.json()
        posts = data.get('data', {}).get('children', [])

        if not posts:
            return f"### r/{subreddit} æœ¬å‘¨æš‚æ— åŒ¹é…ç—›ç‚¹å†…å®¹\n\n"

        report = f"### ğŸ“ r/{subreddit} åŠ¨æ€\n\n"
        for post in posts[:8]: # é€‰å–å‰8æ¡é«˜ç›¸å…³å†…å®¹
            p = post['data']
            # æ—¶é—´è½¬æ¢
            post_time = datetime.fromtimestamp(p['created_utc']).strftime('%Y-%m-%d')

            report += f"#### [{p['title']}](https://reddit.com{p['permalink']})\n"
            report += f"- **å‘å¸ƒæ—¶é—´**: {post_time}\n"
            report += f"- **çƒ­åº¦**: ğŸ‘ {p['score']} | ğŸ’¬ {p['num_comments']} è¯„è®º\n"

            # æˆªå–æ‘˜è¦
            content = p.get('selftext', '')
            if content:
                summary = content[:300].replace('\n', ' ') + "..."
                report += f"- **æ‘˜è¦**: {summary}\n"
            report += "\n---\n"
        return report
    except Exception as e:
        return f"### [!] æŠ“å– r/{subreddit} å‘ç”Ÿè‡´å‘½é”™è¯¯: {str(e)}\n\n"

def fetch_zhihu_pain_points(keyword):
    """
    æŠ“å–çŸ¥ä¹ç›¸å…³è¯é¢˜çš„ç—›ç‚¹è®¨è®º
    æ³¨æ„ï¼šçŸ¥ä¹APIéœ€è¦è®¤è¯ï¼Œæ­¤ç‰ˆæœ¬ä½¿ç”¨æ¨¡æ‹Ÿæœç´¢
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Referer': 'https://www.zhihu.com/'
    }

    # ä½¿ç”¨Googleæœç´¢çŸ¥ä¹å†…å®¹ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆ
    google_search_url = f"https://www.google.com/search?q=site:zhihu.com+{quote(keyword)}"

    try:
        # å°è¯•ç›´æ¥è®¿é—®çŸ¥ä¹æœç´¢é¡µï¼ˆå¯èƒ½éœ€è¦cookieï¼‰
        search_url = f"https://www.zhihu.com/search?type=content&q={quote(keyword)}"
        response = requests.get(search_url, headers=headers, timeout=10)

        if response.status_code == 403 or response.status_code == 400:
            return f"### çŸ¥ä¹ã€Œ{keyword}ã€\n> âš ï¸ çŸ¥ä¹éœ€è¦ç™»å½•è®¤è¯æ‰èƒ½è®¿é—®æœç´¢API\n> ğŸ’¡ å»ºè®®ï¼šä½¿ç”¨çŸ¥ä¹å®˜æ–¹APIæˆ–é…ç½®ç™»å½•Cookie\n> ğŸ”— æ‰‹åŠ¨æœç´¢ï¼š[ç‚¹å‡»è¿™é‡Œåœ¨çŸ¥ä¹æœç´¢ã€Œ{keyword}ã€](https://www.zhihu.com/search?q={quote(keyword)})\n\n"

        if response.status_code != 200:
            return f"### çŸ¥ä¹ã€Œ{keyword}ã€\n> âš ï¸ æš‚æ—¶æ— æ³•è®¿é—® (HTTP {response.status_code})\n> ğŸ”— æ‰‹åŠ¨æœç´¢ï¼š[ç‚¹å‡»è¿™é‡Œåœ¨çŸ¥ä¹æœç´¢ã€Œ{keyword}ã€](https://www.zhihu.com/search?q={quote(keyword)})\n\n"

        # æˆåŠŸè·å–é¡µé¢ï¼Œå°è¯•è§£æï¼ˆç®€åŒ–ç‰ˆï¼‰
        report = f"### çŸ¥ä¹ã€Œ{keyword}ã€\n"
        report += f"> ğŸ”— [åœ¨çŸ¥ä¹æŸ¥çœ‹å®Œæ•´ç»“æœ](https://www.zhihu.com/search?q={quote(keyword)})\n"
        report += "> â„¹ï¸ ç”±äºçŸ¥ä¹APIé™åˆ¶ï¼Œå»ºè®®æ‰‹åŠ¨è®¿é—®ä¸Šè¿°é“¾æ¥æŸ¥çœ‹è¯¦ç»†å†…å®¹\n\n"
        return report

    except Exception as e:
        return f"### çŸ¥ä¹ã€Œ{keyword}ã€\n> âš ï¸ è¿æ¥é”™è¯¯: {str(e)}\n> ğŸ”— [æ‰‹åŠ¨åœ¨çŸ¥ä¹æœç´¢ã€Œ{keyword}ã€](https://www.zhihu.com/search?q={quote(keyword)})\n\n"

def fetch_xiaohongshu_pain_points(keyword):
    """
    æŠ“å–å°çº¢ä¹¦ç›¸å…³è¯é¢˜çš„ç—›ç‚¹è®¨è®º
    æ³¨æ„ï¼šå°çº¢ä¹¦çš„åçˆ¬è™«æœºåˆ¶æå¼ºï¼Œæä¾›æ‰‹åŠ¨æœç´¢é“¾æ¥
    """
    # å°çº¢ä¹¦çš„åçˆ¬è™«æœºåˆ¶åŒ…æ‹¬ï¼šè®¾å¤‡æŒ‡çº¹ã€æ»‘å—éªŒè¯ã€ç™»å½•è¦æ±‚ç­‰
    # ç›´æ¥çˆ¬å–å‡ ä¹ä¸å¯èƒ½ï¼Œæä¾›ç”¨æˆ·å‹å¥½çš„æ›¿ä»£æ–¹æ¡ˆ

    search_url = f"https://www.xiaohongshu.com/search_result?keyword={quote(keyword)}"

    report = f"### å°çº¢ä¹¦ã€Œ{keyword}ã€\n"
    report += f"> ğŸ“± [åœ¨å°çº¢ä¹¦Appä¸­æœç´¢ã€Œ{keyword}ã€]({search_url})\n"
    report += "> â„¹ï¸ å°çº¢ä¹¦éœ€è¦Appç™»å½•æ‰èƒ½æŸ¥çœ‹å†…å®¹ï¼Œå»ºè®®ä½¿ç”¨æ‰‹æœºAppè¿›è¡Œæœç´¢\n"
    report += "> ğŸ’¡ æç¤ºï¼šå¯ä»¥åœ¨å°çº¢ä¹¦Appä¸­æœç´¢å…³é”®è¯ï¼Œå…³æ³¨ç›¸å…³è¯é¢˜çš„ç—›ç‚¹è®¨è®º\n\n"

    return report

def main():
    start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"å¼€å§‹æ‰§è¡Œå…¨çƒæ¢æµ‹ä»»åŠ¡: {start_time}")

    # ç›®æ ‡é¢†åŸŸ
    targets = {
        "CivilEngineering": "åœŸæœ¨å·¥ç¨‹",
        "Construction": "å»ºç­‘æ–½å·¥",
        "QuantitySurveying": "å·¥ç¨‹é€ ä»·/ä¼°ç®—",
        "RealEstate": "æˆ¿åœ°äº§å¼€å‘",
        "PropTech": "åœ°äº§ç§‘æŠ€"
    }

    report_content = f"# ğŸš€ å…¨çƒå»ºç­‘/åœŸæœ¨è¡Œæƒ…ä¸ç—›ç‚¹æ¢æµ‹æŠ¥å‘Š\n\n"
    report_content += f"> **ç”Ÿæˆæ—¶é—´**: {start_time} (UTC)\n"
    report_content += "> **æ¢æµ‹è¯´æ˜**: æœ¬æŠ¥å‘Šè‡ªåŠ¨æ‰«æå…¨çƒç¤¾äº¤åª’ä½“ï¼ˆRedditã€çŸ¥ä¹ã€å°çº¢ä¹¦ï¼‰ï¼Œæå–å…³äºã€Œæµç¨‹ç¹çã€ã€ã€Œæ‰‹åŠ¨æ“ä½œã€åŠã€Œå¯»æ‰¾æ•°å­—åŒ–æ–¹æ¡ˆã€çš„çœŸå®è®¨è®ºã€‚\n\n"

    # å›½é™…å¹³å°ï¼šReddit
    report_content += "# ğŸŒ å›½é™…å¹³å° - Reddit\n\n"
    for sub, name in targets.items():
        print(f"æ­£åœ¨æ‰«æ Reddit: {sub} ({name})...")
        report_content += f"## ğŸ¢ é¢†åŸŸï¼š{name} (r/{sub})\n"
        report_content += fetch_reddit_pain_points(sub)
        time.sleep(2) # ç¤¼è²ŒæŠ“å–é™åˆ¶

    # ä¸­å›½å¹³å°ï¼šçŸ¥ä¹
    report_content += "\n# ğŸ‡¨ğŸ‡³ ä¸­å›½å¹³å° - çŸ¥ä¹\n\n"
    zhihu_keywords = [
        "å»ºç­‘æ–½å·¥ éš¾ç‚¹",
        "åœŸæœ¨å·¥ç¨‹ ç—›ç‚¹",
        "å·¥ç¨‹é€ ä»· æ•ˆç‡",
        "æ–½å·¥ç®¡ç† é—®é¢˜",
        "BIM åº”ç”¨éš¾é¢˜"
    ]

    for keyword in zhihu_keywords:
        print(f"æ­£åœ¨æ‰«æçŸ¥ä¹: {keyword}...")
        report_content += f"## ğŸ” æœç´¢è¯ï¼š{keyword}\n"
        report_content += fetch_zhihu_pain_points(keyword)
        time.sleep(3) # çŸ¥ä¹é™æµè¾ƒä¸¥æ ¼ï¼Œå»¶é•¿é—´éš”

    # ä¸­å›½å¹³å°ï¼šå°çº¢ä¹¦
    report_content += "\n# ğŸ‡¨ğŸ‡³ ä¸­å›½å¹³å° - å°çº¢ä¹¦\n\n"
    xhs_keywords = [
        "å»ºç­‘è®¾è®¡",
        "æ–½å·¥ç°åœº",
        "å·¥ç¨‹ç®¡ç†"
    ]

    for keyword in xhs_keywords:
        print(f"æ­£åœ¨æ‰«æå°çº¢ä¹¦: {keyword}...")
        report_content += f"## ğŸ” æœç´¢è¯ï¼š{keyword}\n"
        report_content += fetch_xiaohongshu_pain_points(keyword)
        time.sleep(3) # å°çº¢ä¹¦é™æµæ›´ä¸¥æ ¼

    # ä¿å­˜æŠ¥å‘Š
    report_file = "PAIN_POINTS_REPORT.md"
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(report_content)

    print(f"ä»»åŠ¡å®Œæˆï¼ŒæŠ¥å‘Šå·²ç”Ÿæˆè‡³ {report_file}")

if __name__ == "__main__":
    main()
